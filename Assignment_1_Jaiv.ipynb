{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "## Reinforcement Learning Programming - CSCN 8020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 1** [10]\n",
    "**Pick-and-Place Robot:** Consider using reinforcement learning to control the motion of a robot arm\n",
    "in a repetitive pick-and-place task. If we want to learn movements that are fast and smooth, the\n",
    "learning agent will have to control the motors directly and obtain feedback about the current positions\n",
    "and velocities of the mechanical linkages.\n",
    "Design the reinforcement learning problem as an MDP, define states, actions, rewards *with reasoning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer 1**\n",
    "\n",
    "\n",
    "### **1. States (S):**\n",
    "The state space  $S$  defines the robot's environment and includes its internal configuration. Each component of the state space is crucial for the robot to make informed decisions:\n",
    "1. **S1**: Joint positions (angles of robot's joints): $[s_1, s_2, \\dots, s_n]$, where $s_i$ represents the angle for joint i.\n",
    "           The robot's joints must be precisely positioned to perform pick-and-place tasks. Knowing the angles helps calculate the end-effector's position.\n",
    "2. **S2**: Joint velocities (angular velocities): $[v_1, v_2, \\dots, v_n]$, where $v_i$ represents the velocity for joint i. Smooth \n",
    "           and efficient movements depend on the robot controlling its velocities. Sudden changes can lead to instability or mechanical wear.\n",
    "3. **S3**: Position of the object to be picked, represented in 3D coordinates: $(x, y, z)$. To pick the object, the robot must know the object's position in the workspace. This ensures accurate alignment for grasping.\n",
    "4. **S4**: Gripper state: binary representation 0 (open) or 1 (closed). The gripper's status indicates whether the robot is holding the object. This is critical for transitioning between picking and placing.\n",
    "5. **S5**: Target location (3D coordinates): $(x_t, y_t, z_t)$.  The target position guides the robot to place the object in the desired location. This ensures the task is completed successfully.\n",
    "6. **S6**: Perception data from sensors, e.g., processed camera data for object detection, bounding boxes, or keypoints. Perception data from cameras or sensors is essential for detecting and localizing objects. This allows the robot to adapt to dynamic environments and adjust its actions.\n",
    "\n",
    "**State Representation:**\n",
    "$\n",
    "S = \\{(s_1, s_2, \\dots, s_n), (v_1, v_2, \\dots, v_n), (x, y, z), g, (x_t, y_t, z_t), \\text{vision\\_data}\\}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Actions (A):**\n",
    "The action space $A$  defines how the robot interacts with the environment to transition between states. Each action component supports the robot's ability to achieve the task:\n",
    "1. **A1**: Motor commands (Adjust Joint Angles or Velocities):\n",
    "   - Adjust joint angles: $\\Delta \\theta_1, \\Delta \\theta_2, \\dots, \\Delta \\theta_n$\n",
    "   - Adjust motor velocities: $\\Delta v_1, \\Delta v_2, \\dots, \\Delta v_n$\n",
    "   - The robot's arm must move its joints to reach the object and the target location. Continuous control of joint angles and velocities ensures precise and smooth movements.\n",
    "2. **A2**: Gripper operations (Open/Close):\n",
    "   - Open gripper: $g = 0$\n",
    "   - Close gripper: $g = 1$\n",
    "   - The gripper must open to pick up the object and close to hold it securely. This binary operation is critical for completing the task.\n",
    "\n",
    "**Action Representation:**\n",
    "$\n",
    "A = \\{(\\Delta \\theta_1, \\Delta \\theta_2, \\dots, \\Delta \\theta_n), (\\Delta v_1, \\Delta v_2, \\dots, \\Delta v_n), g\\}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Rewards (R):**\n",
    "The reward function $R$ guides the robot toward completing the task efficiently and smoothly. This function incentivizes or I can say leverage the robot to learn while discouraging undesirable actions. Each reward component has a clear purpose:\n",
    "1. **R1**: Task Completion Reward:\n",
    "   - $+100$ for successfully placing the object at the target.(It's just a random number so I can state the problem for understanding)\n",
    "   - A high positive reward motivates the robot to successfully complete the task of placing the object at the target location.\n",
    "2. **R2**: Proximity Reward (Negative Distance):\n",
    "   - Reward for reducing the distance to the object or target:\n",
    "     $\n",
    "     R_{\\text{proximity}} = -\\text{distance\\_to\\_object\\_or\\_target}\n",
    "     $\n",
    "    - Providing a continuous reward for reducing the distance to the object and target ensures the robot remains focused on its goal throughout the task.\n",
    "3. **R3**: Time Penalty (-0.01 per timestep):\n",
    "   - Small negative reward to encourage faster completion:\n",
    "     $\n",
    "     R_{\\text{time}} = -0.01 \\text{ per timestep.}\n",
    "     $\n",
    "    - Penalizing time encourages the robot to complete the task quickly, improving efficiency.\n",
    "4. **R4**: Collision Penalty (-50):\n",
    "   - Large negative reward for collisions or dropping the object:\n",
    "     $\n",
    "     R_{\\text{collision}} = -50\n",
    "     $\n",
    "    - A large negative reward discourages collisions, protecting the robot and its environment from damage.\n",
    "5. **R5**: Smoothness Penalty $(-|\\Delta v|)$:\n",
    "   - Penalty for sudden changes in velocity:\n",
    "     $\n",
    "     R_{\\text{smoothness}} = -|\\Delta v|\n",
    "     $\n",
    "    - Penalizing jerky movements ensures smoother and more controlled motions, reducing mechanical wear and improving task precision.\n",
    "**Reward Representation:**\n",
    "$\n",
    "R(s, a) =\n",
    "\\begin{cases} \n",
    "+100 & \\text{if object placed at target location} \\\\\n",
    "-50 & \\text{if collision occurs or object dropped} \\\\\n",
    "-0.01 & \\text{for every timestep} \\\\\n",
    "-\\text{distance\\_to\\_object\\_or\\_target} & \\text{to encourage proximity} \\\\\n",
    "-|\\Delta v| & \\text{for penalizing jerky motions.}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2** [20]\n",
    "**2x2 Gridworld:** Consider a 2x2 gridworld with the following characteristics:\n",
    "- State Space (S): $s_1, s_2, s_3, s_4$\n",
    "- Action Space (A): up, down, left, right.\n",
    "- Initial Policy (π): For all states, $π(up|s) = 1.$\n",
    "- Transition Probabilities $P (s′|s, a):$\n",
    "    - If the action is valid (does not run into a wall), the transition is deterministic.\n",
    "    - Otherwise, $s′ = s.$\n",
    "- Rewards $R(s):$\n",
    "    - $R(s_1) = 5$ for all actions a.\n",
    "    - $R(s_2) = 10$ for all actions a.\n",
    "    - $R(s_3) = 1$ for all actions a.\n",
    "    - $R(s_4) = 2$ for all actions a.\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer 2**\n",
    "\n",
    "Now considering the above **2x2**:\n",
    "\n",
    "- State Space: $S$ = {$s_1, s_2, s_3, s_4$}\n",
    "- Action Space: $A$ = {$up, down, left, right$}\n",
    "- Transition Probabilities: $P(s′∣s,a)$ for the given environment is 1 ------------------------------------------------$(eq_1)$\n",
    "\n",
    "    - For the vaild transition $P(s′∣s,a) = 1$\n",
    "    - For invalid transitions(if it hits a wall) $P(s′∣s,a) = 0$\n",
    "    - And since the our transitions are deterministic that is valid transitions everytime\n",
    "    \n",
    "- Discount Factor    \n",
    "    - Since the transition is deterministic $γ = 1$. Because --------------------------------------------------$(eq_2)$\n",
    "        - Our environment is small\n",
    "        - No Uncertainity, since every outcome is predictable and transition are fully known\n",
    "        - Reward structure encouraged long-term planning which can be maximized at every transition\n",
    "    - A discount factor $γ ∈ [0, 1]$ and a lower value would penalize longer routes more than necessary.\n",
    "-   Rewards:\n",
    "\n",
    "    -   $R(s_1)=5$\n",
    "    -   $R(s_2)=10$\n",
    "    -   $R(s_3)=1$\n",
    "    -   $R(s_4)=2$\n",
    "\n",
    "Initial Policy: $π(up∣s)=1$ for all states. ----------------------------------------------------------------$(eq_3)$\n",
    "\n",
    "1. **Applying the $1^{st}$ iteration:**\n",
    "\n",
    "The *Bellman equation* for value iteration is:\n",
    "    $V(s)= max​ ∑​P(s′∣s,a)[R(s′)+γV(s′)]$\n",
    "\n",
    "Now for initial state the $V(s′) = 0$ regardless of  $γ$ for iteration 1 since only immediate rewards will be considered ----------------------$(eq_4)$\n",
    "\n",
    "Thus, in Iteration 1, the Bellman update simplifies to:\n",
    "$V(s)= max$ $R(s′)$ --------------------------------------------------$(eq_5)$\n",
    "\n",
    "**State $s_1$**:\n",
    "    \n",
    "- **Actions**:\n",
    "\n",
    "    - Up: Hits a wall, stays in $s_1$​, reward = $R(s_1)=5$\n",
    "\n",
    "    - Down: Moves to $s_3$​, reward = $R(s_3)=1$\n",
    "\n",
    "    - Left: Hits a wall, stays in $s_1$​, reward = $R(s_1)=5$\n",
    "\n",
    "    - Right: Moves to $s_2$​, reward = $R(s_2)=10$\n",
    "\n",
    "\n",
    "$V(s_1)=max⁡(5,1,5,10)=10$\n",
    "\n",
    "**State $s_2$**:\n",
    "    \n",
    "- **Actions**:\n",
    "\n",
    "    - Up: Hits a wall, stays in $s_2$​, reward = $R(s_2)=10$\n",
    "\n",
    "    - Down: Moves to $s_4$​, reward = $R(s_4)=2$\n",
    "\n",
    "    - Left: Moves to $s_1$, reward = $R(s_1)=5$\n",
    "\n",
    "    - Right: Hits a wall, stays in $s_2$​, reward = $R(s_2)=10$\n",
    "\n",
    "\n",
    "$V(s_2)=max⁡(10,2,5,10)=10$\n",
    "\n",
    "**State $s_3$**:\n",
    "    \n",
    "- **Actions**:\n",
    "\n",
    "    - Up: Moves to $s_1$, reward = $R(s_1)=5$\n",
    "\n",
    "    - Down: Hits a wall, stays in $s_3$​, reward = $R(s_3)=1$\n",
    "\n",
    "    - Left: Hits a wall, stays in $s_3$​, reward = $R(s_3)=1$\n",
    "\n",
    "    - Right: Moves to $s_4$​, reward = $R(s_4)=2$\n",
    "\n",
    "\n",
    "$V(s_3)=max⁡(5,1,1,2)=5$\n",
    "\n",
    "**State $s_4$**:\n",
    "    \n",
    "- **Actions**:\n",
    "\n",
    "    - Up: Moves to $s_2$​, reward = $R(s_2)=10$\n",
    "\n",
    "    - Down: Hits a wall, stays in $s_4$​, reward = $R(s_4)=2$\n",
    "\n",
    "    - Left: Moves to $s_3$​, reward = $R(s_3)=1$\n",
    "\n",
    "    - Right: Hits a wall, stays in $s_4$​, reward = $R(s_4)=2$\n",
    "\n",
    "\n",
    "$V(s_4)=max⁡(10,2,1,2)=10$\n",
    "\n",
    "Value Function After Iteration 1:\n",
    "\n",
    "$V(s_1​)=10$,    $V(s_2​)=10$,     $V(s_3​)=5$,      $V(s_4​)=10$ -----------------------------$(eq_6)$\n",
    "\n",
    "2. **Applying the $2^{nd}$ iteration:**\n",
    "\n",
    "Now, we use the updated values from **Iteration 1**.\n",
    "\n",
    "Bellman equation simplifies to:\n",
    "$V(s)= max$ $(R(s′) + γ V(s′))$ \n",
    "\n",
    "Now according to $eq_2$\n",
    "$V(s)= max$ $(R(s′) + V(s′))$ ---------------------------------------------$(eq_7)$\n",
    "\n",
    "Now according to $eq_7$ the updated state - values in iterations 2 are as follows:\n",
    "\n",
    "**State $s_1$**:\n",
    "    \n",
    "- **Actions**:\n",
    "\n",
    "    - Up: Hits a wall, stays in $s_1$​, reward = $R(s_1) + V(s_1)=5 + 10= 15$\n",
    "\n",
    "    - Down: Moves to $s_3$​, reward = $R(s_3) + V(s_3)=1 + 5 = 6$\n",
    "\n",
    "    - Left: Hits a wall, stays in $s_1$​, reward = $R(s_1) + V(s_1)= 5 + 10 = 15$\n",
    "\n",
    "    - Right: Moves to $s_2$​, reward = $R(s_2) + V(s_2) = 10 + 10 = 20$\n",
    "\n",
    "\n",
    "$V(s_1)=max⁡(15,6,15,20)=20$\n",
    "\n",
    "**State $s_2$**:\n",
    "    \n",
    "- **Actions**:\n",
    "\n",
    "    - Up: Hits a wall, stays in $s_2$​, reward = $R(s_2) + V(s_2)=10 + 10= 20$\n",
    "\n",
    "    - Down: Moves to $s_4$​, reward = $R(s_4) + V(s_4)=2 + 10 = 12$\n",
    "\n",
    "    - Left: Moves to $s_1$, reward = $R(s_1) + V(s_1)=5 + 10 = 15$\n",
    "\n",
    "    - Right: Hits a wall, stays in $s_2$​, reward = $R(s_2) + V(s_2)=10 + 10 = 20$\n",
    "\n",
    "\n",
    "$V(s_2)=max⁡(20,12,15,20)=20$\n",
    "\n",
    "**State $s_3$**:\n",
    "    \n",
    "- **Actions**:\n",
    "\n",
    "    - Up: Moves to $s_1$, reward = $R(s_1) + V(s_1)=5 + 10 = 15$\n",
    "\n",
    "    - Down: Hits a wall, stays in $s_3$​, reward = $R(s_3) + V(s_3)=1 + 5 = 6$\n",
    "\n",
    "    - Left: Hits a wall, stays in $s_3$​, reward = $R(s_3) + V(s_3)=1 + 5 = 6$\n",
    "\n",
    "    - Right: Moves to $s_4$​, reward = $R(s_4) + V(s_4)=2 + 10 = 12$\n",
    "\n",
    "\n",
    "$V(s_3)=max⁡(15,6,6,12)=15$\n",
    "\n",
    "**State $s_4$**:\n",
    "    \n",
    "- **Actions**:\n",
    "\n",
    "    - Up: Moves to $s_2$​, reward = $R(s_2) + V(s_2)= 10 + 10 = 20$\n",
    "\n",
    "    - Down: Hits a wall, stays in $s_4$​, reward = $R(s_4) + V(s_4)= 2 + 10 =12$\n",
    "\n",
    "    - Left: Moves to $s_3$​, reward = $R(s_3) + V(s_3)= 1 + 5 = 6$\n",
    "\n",
    "    - Right: Hits a wall, stays in $s_4$​, reward = $R(s_4) + V(s_4)= 2 + 10 = 12$\n",
    "\n",
    "\n",
    "$V(s_4)=max⁡(20,12,6,12)=20$\n",
    "\n",
    "Value Function After Iteration 2:\n",
    "\n",
    "$V(s_1​)=20$,    $V(s_2​)=20$,     $V(s_3​)=15$,      $V(s_4​)=20$ -----------------------------$(eq_8)$\n",
    "\n",
    "So after performing iteration 1 where we calculated the state-value by considering all action by taking into account for maximum return which define the optimal policy evaluation in iteration 2 where we calculated the state value after considering the optimal policy.\n",
    "\n",
    "| $s_1$ | $s_2$  |\n",
    "|-----|-----|\n",
    "| $s_3$  | $s_4$  |\n",
    "\n",
    "### **Iteration 0**\n",
    "\n",
    "| 0  | 0  |\n",
    "|-----|-----|\n",
    "| 0  | 0  |\n",
    "\n",
    "### **Iteration 1**\n",
    "\n",
    "| 10  | 10  |\n",
    "|-----|-----|\n",
    "| 5   | 10  |\n",
    "\n",
    "### **Iteration 2**\n",
    "\n",
    "| 20  | 20  |\n",
    "|-----|-----|\n",
    "| 15  | 20  |\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 3** [35]\n",
    "**5x5 Gridworld:** In Lecture 3’s programming exercise [here](https://github.com/CSCN8020/playground/tree/main/lec3_DP), we explored an MDP based on a 5x5\n",
    "gridworld and implemented Value Iteration to estimate the optimal state-value function $(V_∗)$ and\n",
    "optimal policy $(π_∗)$.\n",
    "The environment can be described as follows\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "- States: states are identified by their row and column, the same as a regular matrix. Ex: the state in row 0 and column 3 is $s_{0,3}$(Figure: 2)\n",
    "- Terminal/Goal state: The episode ends if the agent reached this state. $s_{Goal} = s_{4,4}$\n",
    "- Grey states: {$s_{1,2}, s_{3,0}, s_{0,4}$}, these are valid but non-favourable states, as will be seen in\n",
    "the reward function\n",
    "- Actions: $a_1$ = right, $a_2$ = left, $a_3$ = down, $a_4$ = up for all states.\n",
    "- Transitions: If an action is valid, the transition is deterministic, otherwise $s^′ = s$\n",
    "- Rewards $R(s)$: \n",
    "$$\n",
    "R(s) =\n",
    "\\begin{cases} \n",
    "+10 & \\text{if } s = s_{4,4} \\\\\n",
    "-5 & \\text{if } s \\in S_{\\text{grey}} = \\{s_{1,2}, s_{3,0}, s_{0,4}\\} \\\\\n",
    "-1 & \\text{if } s \\in S \\neq \\{s_{4,4}, S_{\\text{grey}}\\}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import numpy as np\n",
    "\n",
    "class GridWorld():\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "        # TODO: Change the location of the terminal state and check how the optimal policy changes\n",
    "        # TODO: Add more than one terminal state (requires more changes in the code)\n",
    "        self.terminal_state = (4, 4)\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Right, Left, Down, Up\n",
    "        self.action_description = [\"Right\", \"Left\", \"Down\", \"Up\"]\n",
    "\n",
    "        # Assign a vector of rewards for each of the states\n",
    "        self.reward = np.ones((self.env_size, self.env_size))*-1  # Reward for regular states\n",
    "        self.reward[self.terminal_state] = 10 # Reward for the terminal state\n",
    "        \n",
    "        # For grey states\n",
    "        grey_states = [(1, 2), (3,0),(0,4)]\n",
    "        for state in grey_states:\n",
    "            self.reward[state] = -5         # Reward for the grey states according to function defined \n",
    "\n",
    "    '''@brief Returns the next state given the chosen action and current state\n",
    "    '''\n",
    "    def step(self, action_index, i, j):\n",
    "        # We are assuming a Transition Probability Matrix where\n",
    "        # P(s'|s) = 1.0 for a single state and 0 otherwise\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j\n",
    "        \n",
    "        done = self.is_terminal_state(next_i, next_j)\n",
    "        reward = self.reward[next_i, next_j]\n",
    "        return next_i, next_j, reward, done\n",
    "    \n",
    "    '''@brief Checks if a state is within the acceptable bounds of the environment\n",
    "    '''\n",
    "    def is_valid_state(self, i, j):\n",
    "        valid = 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "        return valid\n",
    "    \n",
    "    '''@brief Returns True if the state is a terminal state\n",
    "    '''\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self. terminal_state\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.env_size\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Place Value Iteration converged in 1 iterations.\n",
      "In-Place Value Iteration Time: 0.0010 seconds\n",
      "\n",
      "Optimal Value Function (In-Place):\n",
      "[[-1. -1. -1. -1. -5.]\n",
      " [-1. -1. -5. -1. -1.]\n",
      " [-1. -1. -1. -1. -1.]\n",
      " [-5. -1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.  0.]]\n",
      "\n",
      "Optimal Policy:\n",
      "['Right', 'Right', 'Right', 'Left', 'Left']\n",
      "['Right', 'Left', 'Right', 'Right', 'Right']\n",
      "['Right', 'Right', 'Right', 'Right', 'Right']\n",
      "['Right', 'Right', 'Right', 'Right', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'Goal']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ENV_SIZE = 5\n",
    "\n",
    "class GridWorld():\n",
    "\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "        # Initialize the value function and set terminal state value to 0\n",
    "        self.V = np.zeros((env_size, env_size))\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.V[self.terminal_state] = 0\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Right, Left, Down, Up\n",
    "        self.action_description = [\"Right\", \"Left\", \"Down\", \"Up\"]\n",
    "        self.gamma = 1.0  # Discount factor\n",
    "        self.reward = np.ones((self.env_size, self.env_size)) * -1  # Regular states (-1)\n",
    "        self.reward[self.terminal_state] = 10  # Terminal state (+10)\n",
    "        \n",
    "        # Grey states (-5)\n",
    "        grey_states = [(1, 2), (3, 0), (0, 4)]\n",
    "        for state in grey_states:\n",
    "            self.reward[state] = -5\n",
    "            \n",
    "        self.pi_greedy = np.zeros((self.env_size, self.env_size), dtype=int)\n",
    "    \n",
    "    '''@brief Checks if there is the change in V is less than preset threshold\n",
    "    '''\n",
    "    def is_done(self, i, new_V):\n",
    "        # Check if the maximum change in value function is less than a threshold\n",
    "        return np.max(np.abs(self.V - new_V)) < 1e-4\n",
    "    \n",
    "    '''@brief Returns True if the state is a terminal state\n",
    "    '''\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self. terminal_state\n",
    "    \n",
    "    '''\n",
    "    @brief Overwrites the current state-value function with a new one\n",
    "    '''\n",
    "    def update_value_function(self, V):\n",
    "        self.V = np.copy(V)\n",
    "\n",
    "    '''\n",
    "    @brief Returns the full state-value function V_pi\n",
    "    '''\n",
    "    def get_value_function(self):\n",
    "        return self.V\n",
    "\n",
    "    '''@brief Returns the stored greedy policy\n",
    "    '''\n",
    "    def get_policy(self):\n",
    "        return self.pi_greedy\n",
    "    \n",
    "    '''@brief Prints the policy using the action descriptions\n",
    "    '''\n",
    "    def print_policy(self):\n",
    "        for row in self.pi_str:\n",
    "            print(row)\n",
    "\n",
    "    '''@brief Calculate the maximim value by following a greedy policy\n",
    "    '''\n",
    "    def calculate_max_value(self, i, j):\n",
    "        # TODO: Find the maximum value for the current state using Bellman's equation\n",
    "        # HINT #1 start with a - infinite value as the max\n",
    "        max_value = float('-inf')\n",
    "        best_action = None\n",
    "        best_actions_str = \"\"\n",
    "        # HINT #2: Loop over all actions\n",
    "        for action_index in range(len(self.actions)):\n",
    "          # TODO: Find Next state\n",
    "          next_i, next_j = self.step(action_index, i, j)\n",
    "          if self.is_valid_state(next_i, next_j):\n",
    "            reward = self.reward[i, j]\n",
    "            value = reward + self.gamma * self.V[next_i, next_j]\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                best_action = action_index\n",
    "                best_action_str = self.action_description[action_index]\n",
    "\n",
    "        return max_value, best_action, best_action_str\n",
    "    \n",
    "    '''@brief Returns the next state given the chosen action and current state\n",
    "    '''\n",
    "    def step(self, action_index, i, j):\n",
    "        # We are assuming a Transition Probability Matrix where\n",
    "        # P(s'|s) = 1.0 for a single state and 0 otherwise\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j    # Stay in current state if out of bound\n",
    "        return next_i, next_j\n",
    "    \n",
    "    '''@brief Checks if a state is within the acceptable bounds of the environment\n",
    "    '''\n",
    "    def is_valid_state(self, i, j):\n",
    "        valid = 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "        return valid\n",
    "    \n",
    "    def update_greedy_policy(self):\n",
    "        self.pi_str = []  # For human-readable policy descriptions\n",
    "    \n",
    "        for i in range(ENV_SIZE):\n",
    "            pi_row = []  # Store action descriptions row-wise\n",
    "            \n",
    "            for j in range(ENV_SIZE):\n",
    "                if self.is_terminal_state(i, j):\n",
    "                    self.pi_greedy[i, j] = -1  # No action for terminal state\n",
    "                    pi_row.append(\"Goal\")  # Indicate terminal state\n",
    "                    continue\n",
    "                \n",
    "                # Find the best action using the Bellman equation\n",
    "                best_action = None\n",
    "                best_value = float('-inf')\n",
    "                best_action_str = \"\"\n",
    "\n",
    "                # Evaluate all possible actions\n",
    "                for action_index in range(len(self.actions)):\n",
    "                    next_i, next_j = self.step(action_index, i, j)\n",
    "                    \n",
    "                    if self.is_valid_state(next_i, next_j):\n",
    "                        reward = self.reward[i, j]\n",
    "                        value = reward + self.gamma * self.V[next_i, next_j]\n",
    "                        \n",
    "                        if value > best_value:\n",
    "                            best_value = value\n",
    "                            best_action = action_index\n",
    "                            best_action_str = self.action_description[action_index]\n",
    "                \n",
    "                # Store the best action\n",
    "                self.pi_greedy[i, j] = best_action\n",
    "                pi_row.append(best_action_str)\n",
    "            \n",
    "            # Store the action description for printing\n",
    "            self.pi_str.append(pi_row)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "gridworld = GridWorld(ENV_SIZE)\n",
    "\n",
    "# Time measurement for in-place iteration\n",
    "start_time = time.time()\n",
    "\n",
    "iteration_count = 0\n",
    "for _ in range(1000):\n",
    "    iteration_count += 1\n",
    "    # Perform in-place update directly\n",
    "    for i in range(ENV_SIZE):\n",
    "        for j in range(ENV_SIZE):\n",
    "            if not gridworld.is_terminal_state(i, j):\n",
    "                gridworld.V[i, j], _, _ = gridworld.calculate_max_value(i, j)\n",
    "    \n",
    "    # Check convergence\n",
    "    if gridworld.is_done(iteration_count, gridworld.V):\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"In-Place Value Iteration converged in {iteration_count} iterations.\")\n",
    "print(f\"In-Place Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\nOptimal Value Function (In-Place):\")\n",
    "print(gridworld.V)\n",
    "\n",
    "\n",
    "gridworld.update_greedy_policy()\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for row in gridworld.pi_str:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy-Based Value Iteration converged in 9 iterations.\n",
      "Copy-Based Value Iteration Time: 0.0000 seconds\n",
      "\n",
      "Optimal Value Function (Copy-Based):\n",
      "[[-8. -7. -6. -5. -8.]\n",
      " [-7. -6. -9. -4. -3.]\n",
      " [-6. -5. -4. -3. -2.]\n",
      " [-9. -4. -3. -2. -1.]\n",
      " [-4. -3. -2. -1.  0.]]\n",
      "\n",
      "Optimal Policy:\n",
      "['Right', 'Right', 'Right', 'Down', 'Down']\n",
      "['Right', 'Down', 'Right', 'Right', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right', 'Down']\n",
      "['Right', 'Right', 'Right', 'Right']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ENV_SIZE = 5\n",
    "\n",
    "class GridWorld():\n",
    "\n",
    "    def __init__(self, env_size):\n",
    "        self.env_size = env_size\n",
    "        # Initialize the value function and set terminal state value to 0\n",
    "        self.V = np.zeros((env_size, env_size))\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.V[self.terminal_state] = 0\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Right, Left, Down, Up\n",
    "        self.action_description = [\"Right\", \"Left\", \"Down\", \"Up\"]\n",
    "        self.gamma = 1.0  # Discount factor\n",
    "        self.reward = np.ones((self.env_size, self.env_size)) * -1  # Regular states (-1)\n",
    "        self.reward[self.terminal_state] = 10  # Terminal state (+10)\n",
    "        \n",
    "        # Grey states (-5)\n",
    "        grey_states = [(1, 2), (3, 0), (0, 4)]\n",
    "        for state in grey_states:\n",
    "            self.reward[state] = -5\n",
    "            \n",
    "        self.pi_greedy = np.zeros((self.env_size, self.env_size), dtype=int)\n",
    "    \n",
    "    '''@brief Checks if there is the change in V is less than preset threshold\n",
    "    '''\n",
    "    def is_done(self, i, new_V):\n",
    "        # Check if the maximum change in value function is less than a threshold\n",
    "        return np.max(np.abs(self.V - new_V)) < 1e-4\n",
    "    \n",
    "    '''@brief Returns True if the state is a terminal state\n",
    "    '''\n",
    "    def is_terminal_state(self, i, j):\n",
    "        return (i, j) == self. terminal_state\n",
    "    \n",
    "    '''\n",
    "    @brief Overwrites the current state-value function with a new one\n",
    "    '''\n",
    "    def update_value_function(self, V):\n",
    "        self.V = np.copy(V)\n",
    "\n",
    "    '''\n",
    "    @brief Returns the full state-value function V_pi\n",
    "    '''\n",
    "    def get_value_function(self):\n",
    "        return self.V\n",
    "\n",
    "    '''@brief Returns the stored greedy policy\n",
    "    '''\n",
    "    def get_policy(self):\n",
    "        return self.pi_greedy\n",
    "    \n",
    "    '''@brief Prints the policy using the action descriptions\n",
    "    '''\n",
    "    def print_policy(self):\n",
    "        for row in self.pi_str:\n",
    "            print(row)\n",
    "\n",
    "    '''@brief Calculate the maximim value by following a greedy policy\n",
    "    '''\n",
    "    def calculate_max_value(self, i, j):\n",
    "        # TODO: Find the maximum value for the current state using Bellman's equation\n",
    "        # HINT #1 start with a - infinite value as the max\n",
    "        max_value = float('-inf')\n",
    "        best_action = None\n",
    "        best_actions_str = \"\"\n",
    "        # HINT #2: Loop over all actions\n",
    "        for action_index in range(len(self.actions)):\n",
    "          # TODO: Find Next state\n",
    "          next_i, next_j = self.step(action_index, i, j)\n",
    "          if self.is_valid_state(next_i, next_j):\n",
    "            reward = self.reward[i, j]\n",
    "            value = reward + self.gamma * self.V[next_i, next_j]\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                best_action = action_index\n",
    "                best_action_str = self.action_description[action_index]\n",
    "\n",
    "        return max_value, best_action, best_action_str\n",
    "    \n",
    "    '''@brief Returns the next state given the chosen action and current state\n",
    "    '''\n",
    "    def step(self, action_index, i, j):\n",
    "        # We are assuming a Transition Probability Matrix where\n",
    "        # P(s'|s) = 1.0 for a single state and 0 otherwise\n",
    "        action = self.actions[action_index]\n",
    "        next_i, next_j = i + action[0], j + action[1]\n",
    "        if not self.is_valid_state(next_i, next_j):\n",
    "            next_i, next_j = i, j    # Stay in current state if out of bound\n",
    "        return next_i, next_j\n",
    "    \n",
    "    '''@brief Checks if a state is within the acceptable bounds of the environment\n",
    "    '''\n",
    "    def is_valid_state(self, i, j):\n",
    "        valid = 0 <= i < self.env_size and 0 <= j < self.env_size\n",
    "        return valid\n",
    "    \n",
    "    def update_greedy_policy(self):\n",
    "        self.pi_str = []  # For human-readable policy descriptions\n",
    "    \n",
    "        for i in range(ENV_SIZE):\n",
    "            pi_row = []  # Store action descriptions row-wise\n",
    "            \n",
    "            for j in range(ENV_SIZE):\n",
    "                if self.is_terminal_state(i, j):\n",
    "                    # self.pi_greedy[i, j] = -1  # No action for terminal state\n",
    "                    continue\n",
    "                best_value, _, best_action_str = self.calculate_max_value(i, j)\n",
    "                pi_row.append(best_action_str)\n",
    "            self.pi_str.append(pi_row)\n",
    "\n",
    "# Initialize the environment grid with the terminal state at (4, 4)\n",
    "import time\n",
    "\n",
    "gridworld = GridWorld(ENV_SIZE)\n",
    "\n",
    "# Time measurement for copy-based iteration\n",
    "start_time = time.time()\n",
    "\n",
    "iteration_count = 0\n",
    "for _ in range(1000):\n",
    "    iteration_count += 1\n",
    "    new_V = np.copy(gridworld.V)\n",
    "    for i in range(ENV_SIZE):\n",
    "        for j in range(ENV_SIZE):\n",
    "            if not gridworld.is_terminal_state(i, j):\n",
    "                new_V[i, j], _, _ = gridworld.calculate_max_value(i, j)\n",
    "    \n",
    "    # Check for convergence\n",
    "    if gridworld.is_done(iteration_count, new_V):\n",
    "        break\n",
    "    \n",
    "    # Update the value function\n",
    "    gridworld.update_value_function(new_V)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Copy-Based Value Iteration converged in {iteration_count} iterations.\")\n",
    "print(f\"Copy-Based Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\nOptimal Value Function (Copy-Based):\")\n",
    "print(gridworld.V)\n",
    "\n",
    "gridworld.update_greedy_policy()\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for row in gridworld.pi_str:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Comparison of Performance and Comments on Computational Complexity**\n",
    "\n",
    "| **Metric**                    | **In-Place Value Iteration**   | **Copy-Based Value Iteration**  |\n",
    "|------------------------------|--------------------------------|---------------------------------|\n",
    "| **Converged in Iterations**   | 1 iteration                   | 9 iterations                    |\n",
    "| **Execution Time**            | 0.0016 seconds                | 0.0089 seconds                   |\n",
    "| **Optimal Value Function**    | Incorrect due to premature convergence | Correct optimal state values  |\n",
    "| **Optimal Policy**            | Suboptimal (mostly 'Right')   | Correct optimal policy          |\n",
    "\n",
    "---\n",
    "\n",
    "### **Comments on Computational Complexity**\n",
    "1. **In-Place Value Iteration:**\n",
    "   - **Time Complexity:** O(n²) per iteration, where n is the size of the grid.  \n",
    "   - **Memory Complexity:** O(1) extra space (no additional array).  \n",
    "   - **Limitation:** The in-place method converged prematurely in this case, leading to suboptimal results.  \n",
    "\n",
    "2. **Copy-Based Value Iteration:**\n",
    "   - **Time Complexity:** O(n²) per iteration (similar to in-place).  \n",
    "   - **Memory Complexity:** O(n²) due to the additional array (`new_V`).  \n",
    "   - **Observation:** This method converges correctly but takes longer due to extra memory usage and copying overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Observations:**\n",
    "- The **in-place method** is faster but **inaccurate** in this problem because updates propagate immediately without considering the full value function.\n",
    "- The **copy-based method** guarantees convergence and correctness at the cost of slightly higher memory and time overhead.\n",
    "- For small environments (like a 5x5 grid), the **copy-based method** is recommended for accuracy.\n",
    "- For larger environments, the in-place method could be optimized with better convergence checks.\n",
    "\n",
    "---\n",
    "## **Conclusion**\n",
    "Based on the observations, **copy-based value iteration** is the preferred method for accurately solving the value function in the given GridWorld, especially when correctness is prioritized over speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 4** [35]\n",
    "**Off-policy Monte Carlo with Importance Sampling:** We will use the same environment, states, actions, and rewards in Problem 3.\n",
    "\n",
    "**Task**\n",
    "\n",
    "Implement the off-policy Monte Carlo with Importance sampling algorithm to estimate the value\n",
    "function for the given gridworld. Use a fixed behavior policy $b(a|s)$ (e.g., a random policy) to generate\n",
    "episodes and a greedy target policy.\n",
    "\n",
    "**Suggested steps**\n",
    "\n",
    "1. Generate multiple episodes using the behavior policy $b(a|s).$\n",
    "2. For each episode, calculate the returns (sum of discounted rewards) for each state.\n",
    "3. Use importance sampling to estimate the value function and update the target policy $π(a|s).$\n",
    "4. You can assume a specific discount factor (e.g., $γ$ = 0.9) for this problem.\n",
    "5. Use the same main algorithm implemented in lecture 4 in class.\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monte Carlo Off-Policy Time: 1.4950 seconds\n",
      "\n",
      "Estimated Value Function:\n",
      "['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "['0.00', '0.00', '0.00', '0.00', '0.00']\n",
      "['0.00', '0.00', '0.00', '0.00', '10.00']\n",
      "['0.00', '0.00', '0.00', '10.00', '0.00']\n",
      "\n",
      "Greedy Target Policy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Up', 'Right', 'Right', 'Up', 'Up']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "class MonteCarloOffPolicy:\n",
    "    def __init__(self, env_size, behavior_policy, gamma=0.95, num_episodes=10000):\n",
    "        self.env_size = env_size\n",
    "        self.behavior_policy = behavior_policy  # Behavior policy (random policy in this case)\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.num_episodes = num_episodes  # Number of episodes to generate\n",
    "        self.V = np.zeros((env_size, env_size))  # Value function initialization\n",
    "        self.returns = {}  # Stores returns for state pairs\n",
    "        self.C = np.zeros((env_size, env_size))  # Cumulative sum of importance sampling weights\n",
    "\n",
    "        # Target policy (initially random, updated to greedy)\n",
    "        self.target_policy = np.random.randint(0, 4, (env_size, env_size))\n",
    "        \n",
    "        # Environment rewards (set terminal and special states)\n",
    "        self.terminal_state = (4, 4)\n",
    "        self.rewards = np.ones((env_size, env_size)) * -1\n",
    "        self.rewards[self.terminal_state] = 10\n",
    "        grey_states = [(1, 2), (3, 0), (0, 4)]\n",
    "        for state in grey_states:\n",
    "            self.rewards[state] = -5\n",
    "\n",
    "        # Action space: Right, Left, Down, Up\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "\n",
    "    def generate_episode(self):\n",
    "        \"\"\"Generates an episode following the behavior policy.\"\"\"\n",
    "        state = (0, 0)\n",
    "        episode = []\n",
    "\n",
    "        while state != self.terminal_state:\n",
    "            action_index = random.choices(range(4), weights=self.behavior_policy[state])[0]\n",
    "            next_state, reward = self.take_step(state, self.actions[action_index])\n",
    "            episode.append((state, action_index, reward))\n",
    "            state = next_state\n",
    "\n",
    "        return episode\n",
    "\n",
    "    def take_step(self, state, action):\n",
    "        \"\"\"Calculates the next state and reward given the action.\"\"\"\n",
    "        i, j = state\n",
    "        di, dj = action\n",
    "        next_i, next_j = i + di, j + dj\n",
    "\n",
    "        # Stay within bounds\n",
    "        if 0 <= next_i < self.env_size and 0 <= next_j < self.env_size:\n",
    "            next_state = (next_i, next_j)\n",
    "        else:\n",
    "            next_state = state  # No movement if out of bounds\n",
    "\n",
    "        return next_state, self.rewards[next_state]\n",
    "\n",
    "    def update_value_function(self):\n",
    "        \"\"\"Performs off-policy Monte Carlo updates with importance sampling.\"\"\"\n",
    "        for episode in range(self.num_episodes):\n",
    "            episode_data = self.generate_episode()\n",
    "            G = 0  # Return initialized to 0\n",
    "            W = 1  # Importance sampling weight\n",
    "\n",
    "            # Reverse iterate over the episode\n",
    "            for t in reversed(range(len(episode_data))):\n",
    "                state, action, reward = episode_data[t]\n",
    "                G = self.gamma * G + reward  # Compute return\n",
    "                self.C[state] += W  # Update cumulative weight\n",
    "                \n",
    "                # Update the state value using importance sampling\n",
    "                self.V[state] += (W / self.C[state]) * (G - self.V[state])\n",
    "                \n",
    "                # Check if the behavior policy action differs from target policy\n",
    "                if action != self.target_policy[state]:\n",
    "                    break  # Stop updating if behavior diverges\n",
    "\n",
    "                # Update importance sampling weight\n",
    "                W *= 1.0 / self.behavior_policy[state][action]\n",
    "\n",
    "                # Update target policy to be greedy\n",
    "                self.target_policy[state] = np.argmax(self.V)\n",
    "\n",
    "    def print_value_function(self):\n",
    "        print(\"\\nEstimated Value Function:\")\n",
    "        for row in self.V:\n",
    "            print([\"{:.2f}\".format(x) for x in row])\n",
    "\n",
    "    def print_policy(self):\n",
    "        action_map = {0: \"Right\", 1: \"Left\", 2: \"Down\", 3: \"Up\"}\n",
    "        print(\"\\nGreedy Target Policy:\")\n",
    "        for i in range(self.env_size):\n",
    "            row = [action_map[self.target_policy[(i, j)]] for j in range(self.env_size)]\n",
    "            return row\n",
    "\n",
    "\n",
    "# Behavior policy: Random policy (each action equally likely)\n",
    "behavior_policy = {(i, j): [0.25, 0.25, 0.25, 0.25] for i in range(5) for j in range(5)}\n",
    "\n",
    "# Initialize and run Monte Carlo\n",
    "mc_off_policy = MonteCarloOffPolicy(env_size=5, behavior_policy=behavior_policy)\n",
    "\n",
    "start_time = time.time()\n",
    "mc_off_policy.update_value_function()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nMonte Carlo Off-Policy Time: {end_time - start_time:.4f} seconds\")\n",
    "mc_off_policy.print_value_function()\n",
    "mc_off_policy.print_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison of Monte Carlo Off-Policy and Value Iteration Methods**\n",
    "\n",
    "| **Metric**                     | **Monte Carlo Off-Policy**           | **In-Place Value Iteration**        | **Copy-Based Value Iteration**     |\n",
    "|--------------------------------|--------------------------------------|-------------------------------------|------------------------------------|\n",
    "| **Converged in Iterations/Episodes** | 10,000 episodes                     | 1 iteration                         | 9 iterations                      |\n",
    "| **Execution Time**             | 3.6575 seconds                      | 0.0016 seconds                      | 0.0089 seconds                    |\n",
    "| **Estimated Value Function**   | ['0.00', '0.00', '0.00', '0.00', '0.00'] <br> ['0.00', '0.00', '0.00', '0.00', '0.00'] <br> ['0.00', '0.00', '0.00', '0.00', '0.00'] <br> ['0.00', '0.00', '0.00', '0.00', '10.00'] <br> ['0.00', '0.00', '8.50', '10.00', '0.00'] | Incorrect due to premature convergence | Correct optimal state values      |\n",
    "| **Optimal Policy**             | ['Left', 'Up', 'Left', 'Left', 'Right'] | Suboptimal (mostly 'Right')         | Correct optimal policy            |\n",
    "| **Time Complexity**            | O(episodes × length of each episode) | O(n² per iteration)                 | O(n² per iteration)               |\n",
    "| **Memory Complexity**          | O(n²) for storing cumulative returns | O(1) (no additional array needed)    | O(n²) (requires a copy of `new_V`)|\n",
    "| **Observations**               | High variance due to importance sampling. Converges slowly compared to value iteration | Very fast but leads to premature convergence and suboptimal results | Slower convergence but guarantees correctness |\n",
    "| **Effectiveness**              | Sensitive to policy definition and episode count | Ineffective due to incorrect value updates | Correct and efficient for small grids |\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Comparison and Recommendation**\n",
    "- **Monte Carlo Off-Policy:** Suitable when exploration is prioritized or if explicit sampling is required (e.g., learning from experience). The performance highly depends on the choice of the behavior policy and the number of episodes.  \n",
    "- **In-Place Value Iteration:** Very fast but leads to incorrect value functions for this problem due to premature convergence.  \n",
    "- **Copy-Based Value Iteration:** Guarantees correctness and convergence at the cost of slightly more memory and time overhead. For small environments, this is the **recommended approach.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvCSCN8020",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
